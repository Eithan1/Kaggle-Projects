# -*-coding:utf-8-*-
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from keras.models import Sequential  # 导入顺序模型
from keras.layers import Dense, Dropout, Activation, Flatten, MaxPool2D, \
    Conv2D  # 导入全连接层Dense， 激活层Activation 以及 Dropout层
from keras.optimizers import SGD, Adam, RMSprop
from keras.utils.np_utils import to_categorical
from keras.utils.vis_utils import plot_model
from sklearn.model_selection import train_test_split
from keras.callbacks import ReduceLROnPlateau
from keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import confusion_matrix
from sklearn.ensemble import VotingClassifier
import itertools

train_data = pd.read_csv('train.csv')
test_data = pd.read_csv('test.csv')
batch_size = 256  # 在计算力允许的情况下，batch_size越大越好 每次训练和梯度更新块的大小。
nb_classes = 10  # 设置类别的个数
nb_epoch = 30  # 迭代次数

'''数据预处理'''
X_train = train_data.drop(columns=['label'])
Y_train = train_data.label
del train_data
# 改变维度：第一个参数是图片数量，后三个参数是每个图片的维度
X_train = X_train.values.reshape(-1, 28, 28, 1)   #将所有数据分成42000（个图片）*28*28(每个图片)*1（RGB）
test_data = test_data.values.reshape(-1, 28, 28, 1)  # 输入shape转换成了4D tensor，第二与第三维度对应的是照片的宽度与高度，最后一个维度是颜色通道数，本例子中是1
print(X_train.shape)
print(test_data.shape)
print("Train Sample:", X_train.shape[0])
print("Test Sample:", test_data.shape[0])
# 归一化：将数据进行归一化到0-1 因为图像数据最大是255
X_train = X_train / 255.0
test_data = test_data / 255.0
# 将类别向量(从0到nb_classes的整数向量)映射为二值类别矩阵
Y_train = to_categorical(Y_train, num_classes=nb_classes)  # 将标签转化为one-hot编码
X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1)  #训练集中的90%用于学习，10%用于验证(随机划分)
#(X_train,Y_train)构成训练集，（X_val,Yval构成验证集）
plt.imshow(X_train[0][:, :, 0], cmap="Greys")
plt.show()

'''建立模型'''
model = Sequential()  # 初始化一个模型
# filters：卷积核的数目（即输出的维度）
# kernel_size：卷积核的宽度和长度。如为单个整数，则表示在各个空间维度的相同长度。

model.add(Conv2D(filters=32, kernel_size=(5, 5), activation='relu', input_shape=(28, 28, 1)))  # 32个5*5的卷积核   二维卷积层1
model.add(Conv2D(filters=32, kernel_size=(5, 5), activation='relu'))  #二维卷积层2
model.add(MaxPool2D(pool_size=(2, 2)))  # 2*2的池化核尺寸，且采用最大池化的方式  二维池化层1
model.add(Dropout(0.25))  # 每次更新参数时随机断开一定百分比（p）的输入神经元连接，Dropout层用于防止过拟合

model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu')) #二维卷积层3
model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu')) #二维卷积层4
model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2))) #二维池化层2
model.add(Dropout(0.25))
model.add(Flatten()) #Flatten层用来将输入“压平”，即把多维的输入一维化，常用在从卷积层到全连接层的过渡。Flatten不影响batch的大小。
model.add(Dense(256, activation="relu")) #全连接层，output = activation(dot(input, kernel)+bias)

model.add(Dropout(0.5))
model.add(Dense(10, activation="softmax"))  # Softmax函数返回输入属于每一个分类（本例子中是数字）的概率且概率总和为1

'''显示模型信息并保存'''
# print(model.summary())
plot_model(model, to_file='Model.png')

'''数据扩增'''
# 用以生成一个batch的图像数据，支持实时数据提升
datagen = ImageDataGenerator(
    featurewise_center=False,  # set input mean to 0 over the dataset
    samplewise_center=False,  # set each sample mean to 0
    featurewise_std_normalization=False,  # divide inputs by std of the dataset
    samplewise_std_normalization=False,  # divide each input by its std
    zca_whitening=False,  # apply ZCA whitening
    rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)
    zoom_range=0.1,  # Randomly zoom image
    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
    horizontal_flip=False,  # randomly flip images
    vertical_flip=False)  # randomly flip images

# 计算依赖于数据的变换所需要的统计信息(均值方差等),
# 只有使用featurewise_center，featurewise_std_normalization或zca_whitening时需要此函数。
# datagen.fit(X_train)

'''学习率的设置'''
# 当评价指标monitor不在提升时，减少学习率
learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',
                                            patience=3,
                                            verbose=1,
                                            factor=0.5,
                                            min_lr=0.00001)

'''编译和训练模型'''
optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)
# 使用多类的对数损失categorical_crossentropy
model.compile(loss='categorical_crossentropy',
              optimizer=optimizer,
              metrics=['accuracy'])

# history=model.fit(X_train,Y_train,
#           batch_size=batch_size,
#           epochs=nb_epoch,
#           verbose=2,
#           validation_data=(X_val,Y_val))


# ImageDataGenerator需要和fir_generator配合，才能实时进行数据扩增
# 当生成器返回steps_per_epoch次数据时计一个epoch结束，执行下一个epoch
history = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size),
                              epochs=nb_epoch, validation_data=(X_val, Y_val),
                              verbose=2, steps_per_epoch=X_train.shape[0] // batch_size + 1,
                              callbacks=[learning_rate_reduction])

'''评估模型'''
score = model.evaluate(X_val, Y_val, verbose=0)
print('Val loss:', score[0])
print('Val accuracy:', score[1])

'''绘制学习曲线'''
fig, ax = plt.subplots(2, 1)
ax[0].plot(history.history['loss'], color='b', label="Training loss")
ax[0].plot(history.history['val_loss'], color='r', label="validation loss", axes=ax[0])
ax[0].legend(loc='best', shadow=True)
ax[1].plot(history.history['acc'], color='b', label="Training accuracy")
ax[1].plot(history.history['val_acc'], color='r', label="Validation accuracy")
ax[1].legend(loc='best', shadow=True)
plt.show()

'''混淆矩阵'''


def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap="Blues"):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    # 是否进行标准化
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.show()


# 根据验证集标签的真实值和预测值计算混淆矩阵（confusion_matrix）
Y_pred = model.predict(X_val)
Y_pred_classes = np.argmax(Y_pred, axis=1)
Y_true = np.argmax(Y_val, axis=1)
confusion_mtx = confusion_matrix(Y_true, Y_pred_classes)
# 绘制混淆矩阵
plot_confusion_matrix(confusion_mtx, classes=range(10))

'''查看最显著的错误'''


def display_errors(errors_index, img_errors, pred_errors, obs_errors):
    n = 0
    nrows = 2
    ncols = 3
    fig, ax = plt.subplots(nrows, ncols, sharex=True, sharey=True)
    for row in range(nrows):
        for col in range(ncols):
            error = errors_index[n]
            ax[row, col].imshow((img_errors[error]).reshape((28, 28)), cmap="Greys")
            ax[row, col].set_title("Predicted label :{}\nTrue label :{}".format(pred_errors[error], obs_errors[error]))
            n += 1
    plt.show()


errors = (Y_pred_classes - Y_true != 0)  # 矩阵相减得到误差集（?*1）
# 使用布尔索引
Y_pred_classes_errors = Y_pred_classes[errors]  # 误差集的预测标签Y（?*1）
Y_pred_errors = Y_pred[errors]  # 误差集的预测序列（?*10）
Y_true_errors = Y_true[errors]  # 误差集的真实标签Y（?*1）
X_val_errors = X_val[errors]  # 误差集的特征X，即数字图片（?*28*28*1）

# 误差集中对错误标签的预测概率
Y_pred_errors_prob = np.max(Y_pred_errors, axis=1)
# 误差集中对真实标签的预测概率，np.diagonal是返回对角线的元素
true_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))
delta_pred_true_errors = Y_pred_errors_prob - true_prob_errors
# 排序并返回的是数组值从小到大的索引值
sorted_dela_errors = np.argsort(delta_pred_true_errors)
most_important_errors = sorted_dela_errors[-6:]
display_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)

'''预测测试集'''
print('Begin to predict for testing data ...')
results = model.predict(test_data)
results = np.argmax(results, axis=1)
results = pd.Series(results, name="Label")
submission = pd.concat([pd.Series(range(1, 28001), name="ImageId"), results], axis=1)
submission.to_csv("submit.csv", index=False)
